# ══════════════════════════════════════════════════════════════════════════════
# PAGI Ecosystem – Environment Variables
# ══════════════════════════════════════════════════════════════════════════════
# Copy this file to `.env` in the project root.
# All LLM calls are routed through the Rust backend (pagi-gateway).
# The frontend has NO API keys – it only talks to the backend.
#
# Port Ranges (Architecture Standard):
#   Backend/API: 8001-8099
#   Frontend/UI: 3001-3099
# ══════════════════════════════════════════════════════════════════════════════

# ─────────────────────────────────────────────────────────────────────────────
# RUST LOGGING
# ─────────────────────────────────────────────────────────────────────────────
# Log level: error, warn, info, debug, trace
# Can also filter by crate: RUST_LOG=pagi_gateway=debug,pagi_core=info
RUST_LOG=info

# ─────────────────────────────────────────────────────────────────────────────
# PAGI CORE CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────────
# Path to the gateway TOML config (without extension)
# Default: config/gateway (loads config/gateway.toml)
PAGI_CONFIG=config/gateway

# Path to the orchestrator blueprint JSON
# Default: config/blueprint.json
PAGI_BLUEPRINT_PATH=config/blueprint.json

# ─────────────────────────────────────────────────────────────────────────────
# PAGI CONFIG OVERRIDES (PAGI__* pattern)
# ─────────────────────────────────────────────────────────────────────────────
# Any gateway.toml setting can be overridden via PAGI__<KEY>=<value>
# Examples:
# PAGI__PORT=8002
# PAGI__STORAGE_PATH=./custom_data
# PAGI__FRONTEND_ENABLED=false
# PAGI__LLM_MODE=live

# ─────────────────────────────────────────────────────────────────────────────
# LLM PROVIDER (OpenRouter - Default)
# ─────────────────────────────────────────────────────────────────────────────
# LLM invocation mode: "mock" (simulated responses) or "live" (real API calls)
PAGI_LLM_MODE=mock

# OpenRouter API endpoint (default LLM gateway)
# Docs: https://openrouter.ai/docs
PAGI_LLM_API_URL=https://openrouter.ai/api/v1/chat/completions

# OpenRouter API key (required when PAGI_LLM_MODE=live)
# Get yours at: https://openrouter.ai/keys
PAGI_LLM_API_KEY=

# Default model to use via OpenRouter
# Examples: anthropic/claude-3.5-sonnet, openai/gpt-4o, meta-llama/llama-3.1-70b-instruct
PAGI_LLM_MODEL=anthropic/claude-3.5-sonnet

# ─────────────────────────────────────────────────────────────────────────────
# ALTERNATIVE LLM PROVIDERS (Optional)
# ─────────────────────────────────────────────────────────────────────────────
# To use a different provider, change PAGI_LLM_API_URL:
#
# OpenAI Direct:
#   PAGI_LLM_API_URL=https://api.openai.com/v1/chat/completions
#   PAGI_LLM_API_KEY=sk-...
#
# Anthropic Direct:
#   PAGI_LLM_API_URL=https://api.anthropic.com/v1/messages
#   PAGI_LLM_API_KEY=sk-ant-...
#
# Groq (fast inference):
#   PAGI_LLM_API_URL=https://api.groq.com/openai/v1/chat/completions
#   PAGI_LLM_API_KEY=gsk_...
#
# Ollama (local):
#   PAGI_LLM_API_URL=http://localhost:11434/v1/chat/completions
#   PAGI_LLM_API_KEY=   (leave empty)
#
# Together AI:
#   PAGI_LLM_API_URL=https://api.together.xyz/v1/chat/completions
#   PAGI_LLM_API_KEY=...

# ─────────────────────────────────────────────────────────────────────────────
# OPTIONAL: FUTURE INTEGRATIONS
# ─────────────────────────────────────────────────────────────────────────────
# Redis URL (for distributed memory/caching)
# PAGI_REDIS_URL=redis://127.0.0.1:6379

# Vector database URL (for semantic search)
# PAGI_VECTOR_DB_URL=http://127.0.0.1:6333

# Webhook URL for external notifications
# PAGI_WEBHOOK_URL=https://your-webhook-endpoint.com/notify

# ══════════════════════════════════════════════════════════════════════════════
# QUICK START
# ══════════════════════════════════════════════════════════════════════════════
# 1. Copy this file:
#      cp .env.example .env
#
# 2. Add your OpenRouter API key:
#      PAGI_LLM_API_KEY=sk-or-v1-...
#
# 3. Set mode to live:
#      PAGI_LLM_MODE=live
#
# 4. Run pre-flight check:
#      cargo run -p pagi-gateway -- --verify
#
# 5. Start the gateway:
#      cargo run -p pagi-gateway
#
# 6. Start the UI (in another terminal):
#      cd add-ons/pagi-studio-ui/assets/studio-interface && npm run dev
#
# 7. Open http://127.0.0.1:3001
# ══════════════════════════════════════════════════════════════════════════════
